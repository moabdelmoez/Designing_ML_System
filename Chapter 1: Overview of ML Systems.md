In November 2016, Google implemented multilingual neural machine translation into Google Translate. One of the success stories in deploying ML in production at scale.

Many people, when they hear “machine learning system,” think of just the ML algorithms being used such as logistic regression or different types of neural networks. However, the algorithm is only a small part of an ML system in production. The system also includes the business requirements that gave birth to the ML project in the first place, the interface where users and developers interact with your system, the data stack, and the logic for developing, monitoring, and updating your models, as well as the infrastructure that enables the delivery of that logic.

## The relationship between MLOps and System Design;

Ops in MLOps comes from DevOps, short for Developments and Operations. To operationalize something means to bring it into production, which includes deploying, monitoring, and maintaining it. MLOps is a set of tools and best practices for bringing ML into production.
ML systems design takes a system approach to MLOps, which means that it considers an ML system holistically to ensure that all the components and their stakeholders can work together to satisfy the specified objectives and requirements.

## When to use Machine Learning?
Despite an incredible amount of excitement and hype generated by people both inside and outside the field, ML is not a magic tool that can solve all problems. Even for problems that ML can solve, ML solutions might not be the optimal solutions. Before starting an ML project, you might want to ask whether ML is necessary or cost-effective.

Let's examine what ML solutions generally do: Machine learning is an approach to 
(1) learn 
(2) complex patterns from 
(3) existing data and use these patterns to make 
(4) predictions on 
(5) unseen data

ML is also called Software 2.0

In zero-shot learning it’s possible for an ML system to make good predictions for a task without having been trained on data for that task

Without data and without continual learning, many companies follow a “fake-it-till-you make it” approach: launching a product that serves predictions made by humans, instead of ML models, with the hope of using the generated data to train ML models later.

Machine Learning will shine if your problem has these characteristics;
	1- It's repetitive
	2- The cost of wrong predictions is cheap
	3- It's at scale
	4- The patterns are constantly changing
	
If you wait for the technology to prove its worth to the rest of the industry before jumping in, you might end up years or decades behind your competitors.

Difference between ML in research and in production;
![image](https://user-images.githubusercontent.com/37369603/218462269-f8a89c7c-cbbf-4ddf-bad4-3c615c4bf300.png)

There are many stakeholders involved in bringing an ML system into production. Each stakeholder has their own requirements. Having different, often conflicting, requirements can make it difficult to design, develop, and select an ML model that satisfies all the requirements.

## Computational priorities 

When designing an ML system, people who haven’t deployed an ML system often make the mistake of focusing too much on the model development part and not enough on the model deployment and maintenance part.

Research usually prioritizes fast training, whereas production usually prioritizes fast inference.

To reduce latency in production, you might have to reduce the number of queries you can process on the same hardware at a time. If your hardware is capable of processing many more queries at a time, using it to process fewer queries means underutilizing your hardware, increasing the cost of processing each query.

ML algorithms don’t predict the future, but encode the past.

In early 2020, the Turing Award winner Professor Geoffrey Hinton proposed a heatedly debated question about the importance of interpretability in ML systems. “Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90% cure rate and a human surgeon with an 80% cure rate. Do you want the AI surgeon to be illegal?

First, interpretability is important for users, both business leaders and end users, to understand why a decision is made so that they can trust a model and detect potential biases mentioned previously. Second, it’s important for developers to be able to debug and improve a model.
![image](https://user-images.githubusercontent.com/37369603/218463055-210f40a8-3864-4ee0-9f16-4a95d9b3b1d4.png)

